
Construir GitHub Documentaci칩n Lanzamiento de GitHub Pacto de contribuyentes

Procesamiento de lenguaje natural de 칰ltima generaci칩n para PyTorch y TensorFlow 2.0

游뱅Transformers proporciona miles de modelos previamente entrenados para realizar tareas en textos como clasificaci칩n, extracci칩n de informaci칩n, respuesta a preguntas, resumen, traducci칩n, generaci칩n de texto, etc. en m치s de 100 idiomas. Su objetivo es hacer que la PNL de vanguardia sea m치s f치cil de usar para todos.

游뱅Transformers proporciona API para descargar y usar r치pidamente esos modelos previamente entrenados en un texto dado, ajustarlos en sus propios conjuntos de datos y luego compartirlos con la comunidad en nuestro centro de modelos . Al mismo tiempo, cada m칩dulo de Python que define una arquitectura se puede usar de forma independiente y se puede modificar para permitir experimentos de investigaci칩n r치pidos.

游뱅Transformers est치 respaldado por las dos bibliotecas de aprendizaje profundo m치s populares, PyTorch y TensorFlow , con una integraci칩n perfecta entre ellas, lo que le permite entrenar sus modelos con uno y luego cargarlo para inferencia con el otro.

Demos en l칤nea
Puede probar la mayor칤a de nuestros modelos directamente en sus p치ginas desde el centro de modelos . Tambi칠n ofrecemos una API de inferencia para usar esos modelos.

Aqu칤 est치n algunos ejemplos:

Completar palabras enmascaradas con BERT
Reconocimiento de entidad de nombre con Electra
Generaci칩n de texto con GPT-2
Inferencia de lenguaje natural con RoBERTa
Resumen con BART
Respuesta a preguntas con DistilBERT
Traducci칩n con T5
Write With Transformer , creado por el equipo Hugging Face, es la demostraci칩n oficial de las capacidades de generaci칩n de texto de este repositorio.

Tour rapido
Para usar inmediatamente un modelo en un texto dado, proporcionamos la pipelineAPI. Las canalizaciones agrupan un modelo previamente entrenado con el procesamiento previo que se utiliz칩 durante ese entrenamiento del modelo. A continuaci칩n, se explica c칩mo utilizar r치pidamente una canalizaci칩n para clasificar textos positivos y negativos

>> >  de  transformadores de  importaci칩n  de tuber칤as

# Asignar una tuber칤a para el sentimiento-an치lisis 
>> >  clasificador  =  tuber칤a ( 'sentimiento-an치lisis' )
 >> >  clasificador ( 'Estamos muy contentos de incluir la tuber칤a en el dep칩sito de transformadores.' )
[{ 'label' : 'POSITIVO' , 'puntuaci칩n' : 0,9978193640708923 }]
La segunda l칤nea de c칩digo descarga y almacena en cach칠 el modelo preentrenado utilizado por la canalizaci칩n, la tercera l칤nea lo eval칰a en el texto dado. Aqu칤 la respuesta es "positiva" con una confianza del 99,8%.

Este es otro ejemplo de canalizaci칩n que puede extraer respuestas a preguntas de alg칰n contexto:

>> >  de  transformadores de  importaci칩n  de tuber칤as

# Asignar un gasoducto para la pregunta-respuesta 
>> >  question_answerer  =  tuber칤a ( 'de pregunta-respuesta' )
 >> >  question_answerer ({
...      'pregunta' : '쮺u치l es el nombre del repositorio?' ,
...      'context' : 'Pipeline ha sido incluido en el repositorio huggingface / transformers'
...})
{ 'score' : 0.5135612454720828 , 'start' : 35 , 'end' : 59 , 'answer' : 'huggingface / transformers' }
Adem치s de la respuesta, el modelo preentrenado utilizado aqu칤 devolvi칩 su puntuaci칩n de confianza, junto con la posici칩n inicial y la posici칩n final en la oraci칩n tokenizada. Puede obtener m치s informaci칩n sobre las tareas compatibles con la pipelineAPI en este tutorial .

Para descargar y usar cualquiera de los modelos previamente entrenados en su tarea dada, solo necesita usar esas tres l칤neas de c칩digos (versi칩n de PyTorch):

>> >  de  transformadores  importar  AutoTokenizer , Automodel

>> >  tokenizer  =  AutoTokenizer . from_pretrained ( "Bert-base-entubar" )
 >> >  modelo  =  Automodel . from_pretrained ( "bert-base-uncased" )

>> >  entradas  =  tokenizer ( "Hola mundo!" , Return_tensors = "PT" )
 >> >  salidas  =  modelo ( ** entradas )
o para TensorFlow:

>> >  de  transformadores  importar  AutoTokenizer , TFAutoModel

>> >  tokenizer  =  AutoTokenizer . from_pretrained ( "Bert-base-entubar" )
 >> >  modelo  =  TFAutoModel . from_pretrained ( "bert-base-uncased" )

>> >  entradas  =  tokenizer ( "Hola mundo!" , Return_tensors = "TF" )
 >> >  salidas  =  modelo ( ** entradas )
El tokenizador es responsable de todo el preprocesamiento que espera el modelo preentrenado, y se puede llamar directamente en uno (o lista) de textos (como podemos ver en la cuarta l칤nea de ambos ejemplos de c칩digo). Generar치 un diccionario que puede pasar directamente a su modelo (que se hace en la quinta l칤nea).

El modelo en s칤 es un Pytorchnn.Module normal o un TensorFlowtf.keras.Model (dependiendo de su backend) que puede usar normalmente. Por ejemplo, este tutorial explica c칩mo integrar un modelo de este tipo en el ciclo de entrenamiento cl치sico de PyTorch o TensorFlow, o c칩mo usar nuestra TrainerAPI para ajustar r치pidamente en un nuevo conjunto de datos.

쯇or qu칠 deber칤a usar transformadores?
Modelos de 칰ltima generaci칩n f치ciles de usar:

Alto rendimiento en tareas NLU y NLG.
Barrera de entrada baja para educadores y profesionales.
Pocas abstracciones orientadas al usuario con solo tres clases para aprender.
Una API unificada para usar todos nuestros modelos previamente entrenados.
Menores costos de computaci칩n, menor huella de carbono:

Los investigadores pueden compartir modelos entrenados en lugar de siempre volver a capacitarse.
Los profesionales pueden reducir el tiempo de c치lculo y los costos de producci칩n.
Docenas de arquitecturas con m치s de 2000 modelos previamente entrenados, algunos en m치s de 100 idiomas.
Elija el marco adecuado para cada parte de la vida 칰til de un modelo:

Entrene modelos de 칰ltima generaci칩n en 3 l칤neas de c칩digo.
Mueva un solo modelo entre marcos TF2.0 / PyTorch a voluntad.
Elija sin problemas el marco adecuado para la formaci칩n, la evaluaci칩n y la producci칩n.
Personalice f치cilmente un modelo o un ejemplo seg칰n sus necesidades:

Ejemplos de cada arquitectura para reproducir los resultados de los autores oficiales de dicha arquitectura.
Exponga los modelos internos de la forma m치s coherente posible.
Los archivos de modelo se pueden usar independientemente de la biblioteca para experimentos r치pidos.
쯇or qu칠 no deber칤a usar transformadores?
Esta biblioteca no es una caja de herramientas modular de bloques de construcci칩n para redes neuronales. El c칩digo de los archivos del modelo no se refactoriza con abstracciones adicionales a prop칩sito, de modo que los investigadores puedan iterar r치pidamente en cada uno de los modelos sin sumergirse en abstracciones / archivos adicionales.
La API de entrenamiento no est치 dise침ada para funcionar en ning칰n modelo, pero est치 optimizada para funcionar con los modelos proporcionados por la biblioteca. Para bucles gen칠ricos de aprendizaje autom치tico, debe usar otra biblioteca.
Si bien nos esforzamos por presentar tantos casos de uso como sea posible, los scripts en nuestra carpeta de ejemplos son solo eso: ejemplos. Se espera que no funcionen de inmediato en su problema espec칤fico y que se le pedir치 que cambie algunas l칤neas de c칩digo para adaptarlas a sus necesidades.
Instalaci칩n
Con pepita
Este repositorio se prob칩 en Python 3.6+, PyTorch 1.0.0+ (PyTorch 1.3.1+ para ejemplos ) y TensorFlow 2.0.

Deber칤as instalar 游뱅Transformadores en un entorno virtual . Si no est치 familiarizado con los entornos virtuales de Python, consulte la gu칤a del usuario .

Primero, crea un entorno virtual con la versi칩n de Python que vas a usar y act칤valo.

Luego, deber치 instalar al menos uno de TensorFlow 2.0, PyTorch o Flax. Por favor refi칠rase a la p치gina de instalaci칩n TensorFlow , p치gina de instalaci칩n PyTorch en relaci칩n con el comando de instalaci칩n espec칤fica para su plataforma y / o p치gina de instalaci칩n de lino .

Cuando se haya instalado TensorFlow 2.0 y / o PyTorch, 游뱅 Los transformadores se pueden instalar usando pip de la siguiente manera:

pip instalar transformadores
Si desea jugar con los ejemplos, debe instalar la biblioteca desde la fuente .

Con conda
Desde Transformers versi칩n v4.0.0, ahora tenemos un canal Conda: huggingface.

游뱅 Los transformadores se pueden instalar usando conda de la siguiente manera:

conda install -c huggingface transformers
Siga las p치ginas de instalaci칩n de TensorFlow, PyTorch o Flax para ver c칩mo instalarlas con conda.

Arquitecturas de modelos
Todos los puntos de control modelo proporcionados por游뱅Los transformadores se integran a la perfecci칩n desde el centro de modelos huggingface.co , donde los usuarios y las organizaciones los cargan directamente .

N칰mero actual de puntos de control: 

游뱅Transformers actualmente proporciona las siguientes arquitecturas (consulte aqu칤 un resumen de alto nivel de cada una):

ALBERT (de Google Research y el Instituto Tecnol칩gico de Toyota en Chicago) publicado con el art칤culo ALBERT: A Lite BERT for Self-supervised Learning of Language Representations , por Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut.
BART (de Facebook) publicado con el documento BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension por Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov y Luke Zettlemoyer.
BARThez (de 칄cole polytechnique) publicado con el art칤culo BARThez: a Skilled Pretrained French Sequence-to-Sequence Model por Moussa Kamal Eddine, Antoine J.-P. Tixier, Michalis Vazirgiannis.
BERT (de Google) publicado con el documento BERT: Pre-formaci칩n de transformadores bidireccionales profundos para la comprensi칩n del lenguaje por Jacob Devlin, Ming-Wei Chang, Kenton Lee y Kristina Toutanova.
BERT For Sequence Generation (de Google) publicado con el documento Aprovechando los puntos de control pre-entrenados para tareas de generaci칩n de secuencias por Sascha Rothe, Shashi Narayan, Aliaksei Severyn.
Blenderbot (de Facebook) publicado con el papel Recetas para construir un chatbot de dominio abierto por Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Kurt Shuster, Eric M. Smith, Y-Lan Boureau, Jason Weston.
CamemBERT (de Inria / Facebook / Sorbonne) publicado con el art칤culo CamemBERT: a Tasty French Language Model de Louis Martin *, Benjamin Muller *, Pedro Javier Ortiz Su치rez *, Yoann Dupont, Laurent Romary, 칄ric Villemonte de la Clergerie, Djam칠 Seddah y Beno칥t Sagot.
CTRL (de Salesforce) publicado con el documento CTRL: A Conditional Transformer Language Model for Controllable Generation por Nitish Shirish Keskar *, Bryan McCann *, Lav R. Varshney, Caiming Xiong y Richard Socher.
DeBERTa (de Microsoft Research) publicado con el art칤culo DeBERTa: BERT mejorado con decodificaci칩n con atenci칩n desenredada de Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen.
DialoGPT (de Microsoft Research) publicado con el art칤culo DialoGPT: Entrenamiento previo generativo a gran escala para la generaci칩n de respuesta conversacional por Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, Bill Dolan.
DistilBERT (de HuggingFace), publicado junto con el peri칩dico DistilBERT, una versi칩n destilada de BERT: m치s peque침o, m치s r치pido, m치s barato y m치s ligero de Victor Sanh, Lysandre Debut y Thomas Wolf. Se ha aplicado el mismo m칠todo para comprimir GPT2 en DistilGPT2 , RoBERTa en DistilRoBERTa , BERT multiling칲e en DistilmBERT y una versi칩n alemana de DistilBERT.
DPR (de Facebook) publicado con el documento Recuperaci칩n de pasaje denso para respuesta a preguntas de dominio abierto por Vladimir Karpukhin, Barlas O릇z, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen y Wen-tau Yih.
ELECTRA (de Google Research / Stanford University) publicado con el art칤culo ELECTRA: Pre-entrenamiento de codificadores de texto como discriminadores en lugar de generadores por Kevin Clark, Minh-Thang Luong, Quoc V. Le, Christopher D. Manning.
FlauBERT (del CNRS) publicado con el art칤culo FlauBERT: Pre-formaci칩n del modelo ling칲칤stico no supervisado para franc칠s por Hang Le, Lo칦c Vial, Jibril Frej, Vincent Segonne, Maximin Coavoux, Benjamin Lecouteux, Alexandre Allauzen, Beno칥t Crabb칠, Laurent Besacier, Didier Schwab.
Funnel Transformer (de CMU / Google Brain) publicado con el art칤culo Funnel-Transformer: Filtrar la redundancia secuencial para un procesamiento eficiente del lenguaje por Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le.
GPT (de OpenAI) publicado con el art칤culo Improving Language Understanding by Generative Pre-Training por Alec Radford, Karthik Narasimhan, Tim Salimans e Ilya Sutskever.
GPT-2 (de OpenAI) publicado con el papel Los modelos de lenguaje son estudiantes multitarea no supervisados por Alec Radford *, Jeffrey Wu *, Rewon Child, David Luan, Dario Amodei ** e Ilya Sutskever **.
LayoutLM (de Microsoft Research Asia) publicado con el documento LayoutLM: Pre-training of Text and Layout for Document Image Understanding por Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, Ming Zhou.
Longformer (de AllenAI) publicado con el art칤culo Longformer: The Long-Document Transformer por Iz Beltagy, Matthew E. Peters, Arman Cohan.
LXMERT (de UNC Chapel Hill) publicado con el documento LXMERT: Aprendizaje de representaciones de codificador de modalidades cruzadas de Transformers para la respuesta a preguntas de dominio abierto por Hao Tan y Mohit Bansal.
MarianMT Modelos de traducci칩n autom치tica entrenados condatos OPUS por J칬rg Tiedemann. El Marco de Marian est치 siendo desarrollado por el traductor del equipo de Microsoft.
MBart (de Facebook) publicado con el art칤culo Multilingual Denoising Pre-training for Neural Machine Translation por Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, Luke Zettlemoyer.
MPNet (de Microsoft Research) publicado con el documento MPNet: Pre-entrenamiento enmascarado y permutado para la comprensi칩n del lenguaje por Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, Tie-Yan Liu.
MT5 (de Google AI) publicado con el documento mT5: un transformador de texto a texto masivamente multiling칲e y previamente entrenado por Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, Colin Raffel .
Pegasus (de Google) publicado con el art칤culo PEGASUS: Pre-training with Extracted Gap-oraciones para resumen abstracto > por Jingqing Zhang, Yao Zhao, Mohammad Saleh y Peter J. Liu.
ProphetNet (de Microsoft Research) publicado con el art칤culo ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training por Yu Yan, Weizhen Qi, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei Zhang y Ming Zhou .
Reformer (de Google Research) publicado con el art칤culo Reformer: The Efficient Transformer de Nikita Kitaev, 켸ukasz Kaiser, Anselm Levskaya.
RoBERTa (de Facebook), public칩 junto con el art칤culo un enfoque de preentrenamiento BERT robustamente optimizado de Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov. BERT ultiling칲e en DistilmBERT y una versi칩n alemana de DistilBERT.
SqueezeBert publicado con el documento SqueezeBERT: 쯈u칠 puede ense침ar la visi칩n por computadora a la PNL sobre redes neuronales eficientes? por Forrest N. Iandola, Albert E. Shaw, Ravi Krishna y Kurt W. Keutzer.
T5 (de Google AI) publicado con el art칤culo Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer por Colin Raffel y Noam Shazeer y Adam Roberts y Katherine Lee y Sharan Narang y Michael Matena y Yanqi Zhou y Wei Li y Peter J. Liu.
Transformer-XL (de Google / CMU) publicado con el documento Transformer-XL: Modelos de lenguaje atentos m치s all치 de un contexto de longitud fija por Zihang Dai *, Zhilin Yang *, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov.
XLM (de Facebook) publicado junto con el documento Cross-lingual Language Model Pretraining por Guillaume Lample y Alexis Conneau.
XLM-ProphetNet (de Microsoft Research) publicado con el documento ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training por Yu Yan, Weizhen Qi, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei Zhang y Ming Zhou.
XLM-RoBERTa (de Facebook AI), publicado junto con el documento Unsupervised Cross-lingual Representation Learning at Scale por Alexis Conneau *, Kartikay Khandelwal *, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm치n, Edouard Grave, Myle Ott, Luke Zettlemoyer y Veselin Stoyanov.
XLNet (de Google / CMU) publicado con el art칤culo XLNet: Preentrenamiento autorregresivo generalizado para la comprensi칩n del lenguaje por Zhilin Yang *, Zihang Dai *, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le.
쯈uieres aportar un nuevo modelo? Hemos agregado una gu칤a detallada y plantillas para guiarlo en el proceso de agregar un nuevo modelo. Puedes encontrarlos en la templatescarpeta del repositorio. Aseg칰rese de verificar las pautas de contribuci칩n y comunicarse con los mantenedores o abrir un problema para recopilar comentarios antes de comenzar su PR.
Para verificar si cada modelo tiene una implementaci칩n en PyTorch / TensorFlow / Flax o tiene un tokenizador asociado respaldado por el 游뱅Biblioteca de tokenizadores, consulte esta tabla

Estas implementaciones se han probado en varios conjuntos de datos (consulte los scripts de ejemplo) y deben coincidir con el rendimiento de las implementaciones originales. Puede encontrar m치s detalles sobre el rendimiento en la secci칩n Ejemplos de la documentaci칩n .

Aprende m치s
Secci칩n	Descripci칩n
Documentaci칩n	Tutoriales y documentaci칩n completa de API
Resumen de la tarea	Tareas apoyadas por 游뱅 Transformadores
Tutorial de preprocesamiento	Usar la Tokenizerclase para preparar datos para los modelos
Entrenamiento y puesta a punto	Usando los modelos proporcionados por 游뱅Transformers en un ciclo de entrenamiento de PyTorch / TensorFlow y la TrainerAPI
Visita r치pida: scripts de ajuste / uso	Scripts de ejemplo para ajustar modelos en una amplia gama de tareas
Compartir y cargar modelos	Sube y comparte tus modelos perfeccionados con la comunidad
Migraci칩n	Migrar a 游뱅Transformadores de pytorch-transformersopytorch-pretrained-bert
Citaci칩n
Ahora tenemos un documento que puede citar para el游뱅 Biblioteca de transformadores:

@inproceedings { wolf-etal-2020-transformers ,
     title = " Transformers: Procesamiento del lenguaje natural de 칰ltima generaci칩n " ,
     autor = " Thomas Wolf y Lysandre Debut y Victor Sanh y Julien Chaumond y Clement Delangue y Anthony Moi y Pierric Cistac y Tim Rault y R칠mi Louf y Morgan Funtowicz y Joe Davison y Sam Shleifer y Patrick von Platen y Clara Ma y Yacine Jernite y Julien Plu y Canwen Xu y Teven Le Scao y Sylvain Gugger y Mariama Drame y Quentin Lhoest y Alexander M. Rush " ,
     booktitle = "Actas de la Conferencia de 2020 sobre m칠todos emp칤ricos en el procesamiento del lenguaje natural: demostraciones de sistemas " ,
     mes = oct,
     a침o = " 2020 " ,
     direcci칩n = " Online " ,
     editor = " Association for Computational Linguistics " ,
     url = " https: // www .aclweb.org / anthology / 2020.emnlp-demos.6 " ,
     pages = " 38--45 " 
}
